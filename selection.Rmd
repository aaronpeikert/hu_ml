---
title: "Model Selection"
description: |
  Personally I think model selection is the single most important topic in ML. Sadly we had only one session about it, however I might extend this topic beyond the seminars scope, becouse this is where `tidymodels` really shines.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require("pacman"))install.packages("pacman")
pacman::p_load("here", "fs", "tidyverse", "tidymodels", "pander")
theme_set(theme_minimal())
```

# Task 1

1. Load charisma

```{r charisma, layout="l-body-outset"}
charisma <- read_csv(here("data", "raw", "charisma.csv"))
charisma <- charisma %>% filter(., complete.cases(.))
rmarkdown::paged_table(charisma)
```

2. Repeat the linear model also found in [Linear Model](lm.html).

```{r}
lm_single <- lm(charisma ~ avg.speed, data = charisma)
pander(lm_single)
```

3. Predict for `avg.speed=c(0,1,2,3,4,5)` and plot.

```{r, layout="l-body-outset"}
avg.speed <- c(0,1,2,3,4,5)
predictions <- predict(lm_single, newdata = data.frame(avg.speed=avg.speed))
qplot(predictions, avg.speed)
```

# Task 2

1. Write a function which calculates the sum of squares.

```{r}
rss <- function(y, yhat)(y-yhat)^2
```

2. Write a function which calculates the mean squared error.

```{r}
rmse <- function(y, yhat)rss(y, hat)/length(y)
```

3. Test the functions.

I'll delete both and rely on the `yardstick` package, then I don't need testing. I trust Max Kuhn has implemented the rmse correctly :)

```{r}
rm(rmse, rss)
```


# Task 3

1. Load `simulation1.csv`.

```{r}
simulation1 <- read_csv(here("data", "raw", "simulation1.csv"))
```

Note this is **not** the data used in the seminar, however it should be similar.

2. Plot `x` and `y`.

```{r, layout="l-body-outset"}
ggplot(simulation1, aes(x, y)) + geom_point()
```

3. Fit a series of polynomials:
    1. $y = 1$
    2. $y = 1 + x$
    3. $y = 1 + x + x^2$
    4. $y = 1 + x + x^2 + x^3$
    5. $y = 1 + x + x^2 + x^3 + x^4$

```{r}
forms <- list(y ~ 1,
              y ~ 1 + x,
              y ~ 1 + x + I(x^2),
              y ~ 1 + x + I(x^2) + I(x^3),
              y ~ 1 + x + I(x^2) + I(x^3) + I(x^4))

# name them according to there terms
names(forms) <- map(forms, as.character) %>% map_chr(3)

# ignore the following function, you can also use plain lm
# but this function makes the supplied formula transparent
transparent_lm <- function(formula, data, ...){
  eval(bquote(lm(formula = .(formula), data = data, ...)))
}

lms <- map(forms, transparent_lm, simulation1)

pander(lms)
```

```{r}
evaluation <- lms %>%
  map_dfr(augment, .id = "term") %>%
  group_by(term)

pander(rmse(evaluation, y, .fitted), caption = "RMSE on full data")
pander(rsq_trad(evaluation, y, .fitted), caption = "R-square on full data")
```

# Task 4

1. Split the data in 50/50 test- and trainingset.

```{r}
split <- initial_split(simulation1, 1/2)
```

2. Refit models on trainingset and evaluate on testset.

```{r}
# note, this is the exact same code as before with two differences:
# 1. it is in one pipe
# 2. it uses testing(split) & training(split) as data arguments
evaluation_test <- map(forms, transparent_lm, data = training(split)) %>%
  map_dfr(augment, newdata = testing(split), .id = "term") %>%
  group_by(term)
pander(rmse(evaluation_test, y, .fitted), caption = "RMSE on test")

# same but evaluate on training

evaluation_training <- map(forms, transparent_lm, data = training(split)) %>%
  map_dfr(augment, newdata = training(split), .id = "term") %>%
  group_by(term)

pander(rmse(evaluation_training, y, .fitted), caption = "RMSE on training")
```

```{r, layout="l-body-outset"}
bind_rows(testing = evaluation_test,
          training = evaluation_training,
          .id = "sample") %>% 
  group_by(sample, term) %>% 
  rmse(y, .fitted) %>% 
  ggplot(aes(term, .estimate, group = sample, color = sample)) +
  geom_line() + 
  coord_flip() +
  NULL
```

3. Which is the best model?

It is pretty clear (and if not it will be in the next plot), that all models fit bad. The best fitting model however is the $y = 1 + x$ model. That is to be expected because the data is generated by $y = sin(x*1.5) + 0.2*x$.

4. Plot the predictions

```{r, layout="l-body-outset"}
map_dfr(lms,
    augment,
    newdata = tibble(x = seq(-7, 7, length.out = 300)),
    .id = "term") %>% 
  ggplot(aes(x, .fitted, group = term, color = term)) + 
  geom_line() +
  geom_point(aes(x, y), simulation1, inherit.aes = FALSE) +
  ylab("y") +
  NULL
```

# Task 5

Implement five fold cross validation.

```{r, layout="l-body-outset"}
cv5 <- rsample::vfold_cv(simulation1, folds = 5, repeats = 1)

evaluation_cv <- 
  crossing(cv5, terms = names(forms)) %>% # all combinations of models + folds
  mutate(forms = map(terms, ~forms[[.x]]), # get correct formula for model
         lms = map2(forms, splits, ~transparent_lm(.x, analysis(.y))), # fit
         pred = map2(lms, splits, # predict on unseen data aka assesment
                     ~augment(.x, newdata = assessment(.y)))) %>% 
  group_by(id, terms) %>% 
  unnest(pred) %>% 
  rmse(y, .fitted)

evaluation_cv %>% 
  ggplot(aes(id, .estimate, group = terms, color = terms)) +
  geom_line() +
  scale_y_log10() +
  theme(axis.text.x = element_text(angle = 90)) +
  ylab("rmse") +
  NULL

evaluation_cv %>% 
  ggplot(aes(terms, .estimate)) +
  geom_boxplot() +
  scale_y_log10() +
  coord_flip() +
  ylab("rmse") +
  NULL
```

